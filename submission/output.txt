Output for decision tree training algorithm without pruning - Clean data
Average confusion matrix: 
 [[52.4  0.   0.   0.6]
 [ 0.  45.4  2.2  0. ]
 [ 0.2  2.  45.8  0.3]
 [ 0.3  0.   0.1 50.7]]
Accuracy: 0.9715
Precision: [0.98867925 0.95378151 0.94824017 0.99217221]
Recall: [0.9905482  0.95780591 0.95218295 0.98255814]
Macro averaged precision: 0.9707738007624055
Macro averaged recall: 0.9707182837174562
F1-score: [0.98961284 0.95578947 0.95020747 0.98734177]
Macro averaged F1-score: 0.9707378892549594
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
Output for decision tree training algorithm without pruning - Noisy data
Average confusion matrix: 
 [[38.3  3.3  2.8  4.5]
 [ 2.4 40.2  4.2  2.5]
 [ 4.5  3.6 40.5  3.4]
 [ 3.5  2.9  3.2 40.2]]
Accuracy: 0.7959999999999999
Precision: [0.78323108 0.81541582 0.77884615 0.80722892]
Recall: [0.78644764 0.804      0.79881657 0.7944664 ]
Macro averaged precision: 0.7959326524532722
Macro averaged recall: 0.7961804937135998
F1-score: [0.78483607 0.80966767 0.78870497 0.80079681]
Macro averaged F1-score: 0.7960013794897356
=========================================================================
Output for decision tree training algorithm with pruning - Clean data
Average confusion matrix: 
 [[0.46666667 0.         0.         0.        ]
 [0.         0.54444444 0.02222222 0.        ]
 [0.         0.02222222 0.65555556 0.        ]
 [0.         0.         0.01111111 0.5       ]]
Accuracy: 0.9749999999999999
Precision: [1.         0.96078431 0.96721311 0.97826087]
Recall: [1.         0.96078431 0.9516129  1.        ]
Macro averaged precision: 0.9780993042378241
Macro averaged recall: 0.9765645745112015
F1-score: [1.         0.96078431 0.95934959 0.98901099]
Macro averaged F1-score: 0.9772862240581035
average pre-pruning depth 13.0
average post-pruning depth 10.0
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
Output for decision tree training algorithm with pruning - Noisy data
Average confusion matrix: 
 [[0.34444444 0.01111111 0.02222222 0.01111111]
 [0.         0.45555556 0.07777778 0.03333333]
 [0.         0.         0.52222222 0.03333333]
 [0.         0.04444444 0.01111111 0.65555556]]
Accuracy: 0.89
Precision: [0.88571429 0.80392157 0.94       0.921875  ]
Recall: [1.         0.89130435 0.8245614  0.89393939]
Macro averaged precision: 0.9024512863185633
Macro averaged recall: 0.8878777135854342
F1-score: [0.93939394 0.84536082 0.87850467 0.90769231]
Macro averaged F1-score: 0.8927379361814278
average pre-pruning depth 19.0
average post-pruning depth 13.0
=========================================================================
(base) MacBook-Air-63:submission matisbodaghotmail.fr$ python main.py 
Output for decision tree training algorithm without pruning - Clean data
Average confusion matrix: 
 [[52.4  0.   0.   0.6]
 [ 0.  45.4  2.2  0. ]
 [ 0.2  2.  45.8  0.3]
 [ 0.3  0.   0.1 50.7]]
Accuracy: 0.9715
Precision: [0.98867925 0.95378151 0.94824017 0.99217221]
Recall: [0.9905482  0.95780591 0.95218295 0.98255814]
Macro averaged precision: 0.9707738007624055
Macro averaged recall: 0.9707182837174562
F1-score: [0.98961284 0.95578947 0.95020747 0.98734177]
Macro averaged F1-score: 0.9707378892549594
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
Output for decision tree training algorithm without pruning - Noisy data
Average confusion matrix: 
 [[38.3  3.3  2.8  4.5]
 [ 2.4 40.2  4.2  2.5]
 [ 4.5  3.6 40.5  3.4]
 [ 3.5  2.9  3.2 40.2]]
Accuracy: 0.7959999999999999
Precision: [0.78323108 0.81541582 0.77884615 0.80722892]
Recall: [0.78644764 0.804      0.79881657 0.7944664 ]
Macro averaged precision: 0.7959326524532722
Macro averaged recall: 0.7961804937135998
F1-score: [0.78483607 0.80966767 0.78870497 0.80079681]
Macro averaged F1-score: 0.7960013794897356
=========================================================================
Output for decision tree training algorithm with pruning - Clean data
Average confusion matrix: 
 [[49.3         0.          0.11111111  0.08888889]
 [ 0.         48.3         2.          0.        ]
 [ 0.78888889  2.08888889 47.12222222  0.5       ]
 [ 0.32222222  0.          0.43333333 48.94444444]]
Accuracy: 0.9683333333333334
Precision: [0.9959596  0.96023857 0.93311331 0.98479767]
Recall: [0.977959   0.95854465 0.94876957 0.98811126]
Macro averaged precision: 0.968346123011814
Macro averaged recall: 0.9685272877044295
F1-score: [0.98687722 0.95939086 0.94087632 0.98645169]
Macro averaged F1-score: 0.9683990223835304
average pre-pruning depth 11.844444444444445
average post-pruning depth 8.533333333333335
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
Output for decision tree training algorithm with pruning - Noisy data
Average confusion matrix: 
 [[44.07777778  1.05555556  2.05555556  2.31111111]
 [ 2.14444444 43.96666667  3.27777778  2.21111111]
 [ 2.14444444  3.43333333 44.62222222  1.8       ]
 [ 2.82222222  0.82222222  2.12222222 41.13333333]]
Accuracy: 0.8690000000000001
Precision: [0.89046016 0.85206718 0.85811966 0.87704335]
Recall: [0.86108096 0.89222097 0.85683806 0.86677593]
Macro averaged precision: 0.869228981683302
Macro averaged recall: 0.8694225883410773
F1-score: [0.87552417 0.8716819  0.85747838 0.87187942]
Macro averaged F1-score: 0.8691409669042558
average pre-pruning depth 17.96666666666666
average post-pruning depth 13.977777777777778